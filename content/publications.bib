@article{li2025speckle2self,
  selected={true},
  title={Speckle2Self: Self-supervised ultrasound speckle reduction without clean data},
  author={Li, Xuesong and Navab, Nassir and Jiang, Zhongliang*},
  journal={Medical Image Analysis},
  pages={103755},
  year={2025},
  publisher={Elsevier},
  preview={imgs_paper/speckle2self_Overview.png},
  doi = {10.1016/j.media.2025.103755},
  keywords={Ultrasound Imaging, Image Denoising, Self-supervised Learning, Speckle Reduction},
  abstract = {Image denoising is a fundamental task in computer vision, particularly in medical ultrasound (US) imaging, where speckle noise significantly degrades image quality. Although recent advancements in deep neural networks have led to substantial improvements in denoising for natural images, these methods cannot be directly applied to US speckle noise, as it is not purely random. Instead, US speckle arises from complex wave interference within the body microstructure, making it tissue-dependent. This dependency means that obtaining two independent noisy observations of the same scene, as required by pioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also cannot handle US speckle noise due to its high spatial dependency. To address this challenge, we introduce Speckle2Self, a novel self-supervised algorithm for speckle reduction using only single noisy observations. The key insight is that applying a multi-scale perturbation (MSP) operation introduces tissue-dependent variations in the speckle pattern across different scales, while preserving the shared anatomical structure. This enables effective speckle suppression by modeling the clean image as a low-rank signal and isolating the sparse noise component. To demonstrate its effectiveness, Speckle2Self is comprehensively compared with conventional filter-based denoising algorithms and SOTA learning-based methods, using both realistic simulated US images and human carotid US images. Additionally, data from multiple US machines are employed to evaluate model generalization and adaptability to images from unseen domains. Project page: https://noseefood.github.io/us-speckle2self/},
  description={A novel self-supervised speckle reduction method in ultrasound imaging via multi-scale perturbations transformation, enabling effective despeckling without clean reference data.}
}

@inproceedings{li2025semantic,
  selected={true},
  title={Semantic scene graph for ultrasound image explanation and scanning guidance},
  author={Li, Xuesong and Huang, Dianye and Zhang, Yameng and Navab, Nassir and Jiang, Zhongliang*},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={500--510},
  year={2025},
  organization={Springer},
  langid = {english},
  doi = {10.1007/978-3-032-05114-1_48},
  keywords={Ultrasound Imaging, Scene Graph, Image Explanation, Scanning Guidance, Large Language Models},
  abstract = {Understanding medical ultrasound imaging remains a long-standing challenge due to significant visual variability caused by differences in imaging and acquisition parameters. Recent advancements in large language models (LLMs) have been used to automatically generate terminology-rich summaries orientated to clinicians with sufficient physiological knowledge. Nevertheless, the increasing demand for improved ultrasound interpretability and basic scanning guidance among non-expert users, e.g., in point-of-care settings, has not yet been explored. In this study, we first introduce the scene graph (SG) for ultrasound images to explain image content to non-expert users and provide guidance for ultrasound scanning. The ultrasound SG is first computed using a transformer-based one-stage method, eliminating the need for explicit object detection. To generate a graspable image explanation for non-expert users, the user query is then used to further refine the abstract SG representation through LLMs. Additionally, the predicted SG is explored for its potential in guiding ultrasound scanning toward missing anatomies within the current imaging view, assisting ordinary users in achieving more standardized and complete anatomical exploration. The effectiveness of this SG-based image explanation and scanning guidance has been validated on images from the left and right neck regions, including the carotid and thyroid, across five volunteers. The results demonstrate the potential of the method to maximally democratize ultrasound by enhancing its interpretability and usability for non-expert users.},
  description={A novel approach to ultrasound image analysis using semantic scene graphs for improved explanation and guidance during scanning procedures.}
}

@article{chen2025vibration,
  title={Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound (IROS 2025)},
  author={Chen, Zhongyu and Li, Chenyang and Li, Xuesong and Huang, Dianye and Jiang, Zhongliang and Speidel, Stefanie and Chu, Xiangyu* and Au, KW},
  journal={arXiv preprint arXiv:2508.06921},
  year={2025},
  langid = {english},
  keywords={Robotic Ultrasound, Needle Alignment, Vibration Sensing, Medical Robotics},
  description={A novel approach for restoring needle alignment in autonomous robotic ultrasound using vibration-based energy metrics.},
  abstract = {Precise needle alignment is essential for percutaneous needle insertion in robotic ultrasound-guided procedures. However, inherent challenges such as speckle noise, needle-like artifacts, and low image resolution complicate robust needle detection, which is essential for alignment in ultrasound images. These issues become particularly problematic when visibility is reduced or lost, diminishing the effectiveness of visual-based needle alignment methods. In this paper, we propose a method to restore effectively when the ultrasound imaging plane and the needle insertion plane are misaligned. Unlike many existing approaches that rely heavily on needle visibility in ultrasound images, our method uses a more robust feature by periodically vibrating the needle using a mechanical system. Specifically, we propose a new vibration-based energy metric that remains effective even when the needle is fully out of plane. Using this metric, we develop an elegant control strategy to reposition the ultrasound probe in response to misalignments between the imaging plane and the needle insertion plane in both translation and rotation. Experiments conducted on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided needle insertion system demonstrate the effectiveness of the proposed approach. The experimental results show the translational error of 0.41±0.27 mm and the rotational error of 0.51±0.19 degrees.}
}

@article{jiang2024needle,
  title={Needle segmentation using gan: Restoring thin instrument visibility in robotic ultrasound},
  author={Jiang, Zhongliang# and Li, Xuesong# and Chu, Xiangyu* and Karlas, Angelos and Bi, Yuan and Cheng, Yingsheng* and Au, KW Samuel and Navab, Nassir},
  journal={IEEE Transactions on Instrumentation and Measurement},
  year={2024},
  publisher={IEEE},
  langid = {english},
  doi = {10.1109/TIM.2024.3451569},
  description={A novel GAN-based approach for accurate needle segmentation in robotic ultrasound, enhancing visibility of thin instruments during procedures.},
  abstract = {Ultrasound-guided percutaneous needle insertion is a standard procedure employed in both biopsy and ablation in clinical practices. However, due to the complex interaction between tissue and instrument, the needle may deviate from the in-plane view, resulting in a lack of close monitoring of the percutaneous needle. To address this challenge, we introduce a robot-assisted ultrasound (US) imaging system designed to seamlessly monitor the insertion process and autonomously restore the visibility of the inserted instrument when misalignment happens. To this end, the adversarial structure is presented to encourage the generation of segmentation masks that align consistently with the ground truth in high-order space. This study also systematically investigates the effects on segmentation performance by exploring various training loss functions and their combinations. When misalignment between the probe and the percutaneous needle is detected, the robot is triggered to perform transverse searching to optimize the positional and rotational adjustment to restore needle visibility. The experimental results on ex-vivo porcine samples demonstrate that the proposed method can precisely segment the percutaneous needle (with a tip error of 0.37±0.29mm and an angle error of 1.19±0.29◦ ). Furthermore, the needle appearance can be successfully restored under the repositioned probe pose in all 45 trials, with repositioning errors of 1.51 ± 0.95mm and 1.25 ± 0.79◦ .}
}

@article{jiang2024class,
  title={Class-aware cartilage segmentation for autonomous US-CT registration in robotic intercostal ultrasound imaging},
  author={Jiang, Zhongliang* and Kang, Yunfeng and Bi, Yuan and Li, Xuesong and Li, Chenyang and Navab, Nassir},
  journal={IEEE Transactions on Automation Science and Engineering},
  volume={22},
  pages={4818--4830},
  year={2024},
  publisher={IEEE},
  langid = {english},
  doi = {10.1109/TASE.2024.3411784},
  description={A class-aware cartilage segmentation method for enhancing autonomous ultrasound-CT registration in robotic intercostal imaging.},
  abstract = {Ultrasound imaging has been widely used in clinical examinations owing to the advantages of being portable, realtime, and radiation-free. Considering the potential of extensive deployment of autonomous examination systems in hospitals, robotic US imaging has attracted increased attention. However, due to the inter-patient variations, it is still challenging to have an optimal path for each patient, particularly for thoracic applications with limited acoustic windows, e.g., intercostal liver imaging. To address this problem, a class-aware cartilage bone segmentation network with geometry-constraint post-processing is presented to capture patient-specific rib skeletons. Then, a dense skeleton graph-based non-rigid registration is presented to map the intercostal scanning path from a generic template to individual patients. By explicitly considering the high-acoustic impedance bone structures, the transferred scanning path can be precisely located in the intercostal space, enhancing the visibility of internal organs by reducing the acoustic shadow. To evaluate the proposed approach, the final path mapping performance is validated on five distinct CTs and two volunteer US data, resulting in ten pairs of CT-US combinations. Results demonstrate that the proposed graph-based registration method can robustly and precisely map the path from CT template to individual patients (Euclidean error: 2.21 ± 1.11 mm).}
}

@inproceedings{jiang2023thoracic,
  title={Thoracic cartilage ultrasound-ct registration using dense skeleton graph},
  author={Jiang, Zhongliang* and Li, Chenyang and Li, Xuesong and Navab, Nassir},
  booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={6586--6592},
  langid = {english},
  year={2023},
  organization={IEEE},
  doi = {10.1109/IROS55552.2023.10341575},
  description={A dense skeleton graph-based method for thoracic cartilage ultrasound-CT registration, improving accuracy and robustness in interventional procedures.},
  abstract = {Autonomous ultrasound (US) imaging has gained increased interest recently, and it has been seen as a potential solution to overcome the limitations of free-hand US examinations, such as inter-operator variations. However, it is still challenging to accurately map planned paths from a generic atlas to individual patients, particularly for thoracic applications with high acoustic-impedance bone structures under the skin. To address this challenge, a graph-based non-rigid registration is proposed to enable transferring planned paths from the atlas to the current setup by explicitly considering subcutaneous bone surface features instead of the skin surface. To this end, the sternum and cartilage branches are segmented using a template matching to assist coarse alignment of US and CT point clouds. Afterward, a directed graph is generated based on the CT template. Then, the self-organizing map using geographical distance is successively performed twice to extract the optimal graph representations for CT and US point clouds, individually. To evaluate the proposed approach, five cartilage point clouds from distinct patients are employed. The results demonstrate that the proposed graph-based registration can effectively map trajectories from CT to the current setup for displaying US views through limited intercostal space. The non-rigid registration results in terms of Hausdorff distance (Mean±SD) is 9.48±0.27 mm and the path transferring error in terms of Euclidean distance is 2.21 ± 1.11 mm.}
}

@article{jiang2023skeleton,
  title={Skeleton graph-based ultrasound-CT non-rigid registration},
  author={Jiang, Zhongliang* and Li, Xuesong and Zhang, Chenyu and Bi, Yuan and Stechele, Walter and Navab, Nassir},
  journal={IEEE Robotics and Automation Letters},
  volume={8},
  number={8},
  pages={4394--4401},
  langid = {english},
  year={2023},
  publisher={IEEE},
  doi = {10.1109/LRA.2023.3281267},
  description={A skeleton graph-based approach for non-rigid registration between ultrasound and CT images, enhancing alignment accuracy in medical imaging.},
  abstract = {Autonomous ultrasound (US) scanning has attracted increased attention, and it has been seen as a potential solution to overcome the limitations of conventional US examinations, such as inter-operator variations. However, it is still challenging to autonomously and accurately transfer a planned scan trajectory on a generic atlas to the current setup for different patients, particularly for thorax applications with limited acoustic windows. To address this challenge, we proposed a skeleton graph-based non-rigid registration to adapt patient-specific properties using subcutaneous bone surface features rather than the skin surface. To this end, the self-organization mapping is successively used twice to unify the input point cloud and extract the key points, respectively. Afterward, the minimal spanning tree is employed to generate a tree graph to connect all extracted key points. To appropriately characterize the rib cartilage outline to match the source and target point cloud, the path extracted from the tree graph is optimized by maximally maintaining continuity throughout each rib. To validate the proposed approach, we manually extract the US cartilage point cloud from one volunteer and seven CT cartilage point clouds from different patients. The results demonstrate that the proposed graph-based registration is more effective and robust in adapting to the inter-patient variations than the ICP (distance error mean±SD: 5.0±1.9 mm vs 8.6 ± 6.7 mm on seven CTs).}
}

